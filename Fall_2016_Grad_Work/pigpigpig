d $HADOOP_PREFIX 
$ ./hd_start

pig

bin


/* loading files into HDFS */

$ bin/hdfs dfs -mkdir /PigFinal/Amazon


bin/hdfs dfs -copyFromLocal /media/sf_FilesFromHDFS/AmazonSentiment.csv /PigFinal1/Amazon1





########

/* load data */


amazon = LOAD '/PigFinal/Amazon/AmazonSentiment.txt' USING PigStorage('\t') AS  (Sentence:chararray, Sentiment:int);


/*Isolate sentiment */

H = foreach amazon GENERATE Sentiment;


/* Filter for 1 value */


Sentimentone = filter amazon by Sentiment == 1;



LOGS_GROUP = GROUP Sentimentone ALL;

LOG_COUNT_2 = FOREACH LOGS_GROUP GENERATE COUNT(Sentimentone.Sentiment);


#500


/* Filter for 0 value */


Sentimentzero = filter amazon by Sentiment == 0;



LOGS_GROUP_4 = GROUP Sentimentzero ALL;

LOG_COUNT_4 = FOREACH LOGS_GROUP_4 GENERATE COUNT(Sentimentzero.Sentiment);


_______ / *already subsetted to find  top 10 words that are long based on "0" for negatives sentences */

tokenizezero = foreach Sentimentzero generate flatten(Sentence) as sentence;


flattenzero = foreach tokenizezero  generate flatten(Sentimentzero.Sentence) as sent;


gw = group flattenzero by sent;


sumw = foreach gw generate group, COUNT(flattenzero.sent) as sumword;

sumord = order sumw by sumword DESC;



____________ __ / *already subsetted to find  top 10 words that are long based on "1" for negatives sentences */


tokenizeone = foreach Sentimentone generate flatten(Sentence) as sentence;


flattenone = foreach tokenizeone generate flatten(Sentimentzero.Sentence) as senta;


gw = group flattenone by sent;


sumwa = foreach gw generate group, COUNT(flattenone.senta) as sumword;

sumorda = order sumwa by sumword DESC;



_____________











